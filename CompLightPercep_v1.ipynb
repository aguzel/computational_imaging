{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CompLightPercep_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOs9d+iqpe5rWBM/DatGGB4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguzel/computational_imaging/blob/main/CompLightPercep_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lHTqF_kcuwC1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np_cpu\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(fn):\n",
        "    \"\"\"\n",
        "    Definition to load an image from a given location as a Numpy array.\n",
        "    Parameters\n",
        "    ----------\n",
        "    fn           : str\n",
        "                   Filename.\n",
        "    Returns\n",
        "    ----------\n",
        "    image        :  ndarray\n",
        "                    Image loaded as a Numpy array.\n",
        "    \"\"\"\n",
        "    image = Image.open(fn)\n",
        "    image = np_cpu.array(image)\n",
        "    image = torch.from_numpy(image)\n",
        "    return image"
      ],
      "metadata": {
        "id": "iA12TFsyyh9F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb_2_ycrcb(image):\n",
        "    \"\"\"\n",
        "    Converts an image from RGB colourspace to YCrCb colourspace.\n",
        "    Parameters\n",
        "    ----------\n",
        "    image   : torch.tensor\n",
        "                Input image. Should be an RGB floating-point image with values in the range [0, 1]\n",
        "                Should be in NCHW format.\n",
        "    Returns\n",
        "    -------\n",
        "    ycrcb   : torch.tensor\n",
        "                Image converted to YCrCb colourspace.\n",
        "    \"\"\"\n",
        "    ycrcb = torch.zeros(image.size()).to(image.device)\n",
        "    ycrcb[:, 0, :, :] = 0.299 * image[:, 0, :, :] + 0.587 * \\\n",
        "        image[:, 1, :, :] + 0.114 * image[:, 2, :, :]\n",
        "    ycrcb[:, 1, :, :] = 0.5 + 0.713 * (image[:, 0, :, :] - ycrcb[:, 0, :, :])\n",
        "    ycrcb[:, 2, :, :] = 0.5 + 0.564 * (image[:, 2, :, :] - ycrcb[:, 0, :, :])\n",
        "    return ycrcb"
      ],
      "metadata": {
        "id": "w9Q6ssh44ERv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Im = Image.open('/content/image2.png')\n",
        "# Im"
      ],
      "metadata": {
        "id": "3QXSA7z2uyKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_image_1 = load_image('/content/image2.png')\n",
        "tensor_image_1 = tensor_image_1/255.0\n",
        "tensor_image_2 = load_image('/content/image2.png')\n",
        "tensor_image_2 = tensor_image_2/255.0"
      ],
      "metadata": {
        "id": "g2BJieiqy3u8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nchw_image_1 = torch.swapaxes(tensor_image_1,0,2)\n",
        "nchw_image_2 = torch.swapaxes(tensor_image_2,0,2)\n",
        "print(nchw_image_1.unsqueeze(0).shape) # Add batch dimension for NCHW, prints \"torch.Size([1, 3, 2, 2])\"\n",
        "print(nchw_image_2.unsqueeze(0).shape) # Add batch dimension for NCHW, prints \"torch.Size([1, 3, 2, 2])\"\n",
        "nchw_image_1 = nchw_image_1.unsqueeze(0)\n",
        "nchw_image_2 = nchw_image_2.unsqueeze(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cq_y3t76csk",
        "outputId": "38fbc2b5-b42b-45c1-d492-990ec9360144"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 1001, 1001])\n",
            "torch.Size([1, 3, 1001, 1001])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ycrcb_image_1 = rgb_2_ycrcb(nchw_image_1)\n",
        "ycrcb_image_2 = rgb_2_ycrcb(nchw_image_2)"
      ],
      "metadata": {
        "id": "iVS7k1-17QoD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ycrcb_2_rgb(image):\n",
        "    \"\"\"\n",
        "    Converts an image from YCrCb colourspace to RGB colourspace.\n",
        "    Parameters\n",
        "    ----------\n",
        "    image   : torch.tensor\n",
        "                Input image. Should be a YCrCb floating-point image with values in the range [0, 1]\n",
        "                Should be in NCHW format.\n",
        "    Returns\n",
        "    -------\n",
        "    rgb     : torch.tensor\n",
        "                Image converted to RGB colourspace.\n",
        "    \"\"\"\n",
        "    rgb = torch.zeros(image.size(), device=image.device)\n",
        "    rgb[:, 0, :, :] = image[:, 0, :, :] + 1.403 * (image[:, 1, :, :] - 0.5)\n",
        "    rgb[:, 1, :, :] = image[:, 0, :, :] - 0.714 * \\\n",
        "        (image[:, 1, :, :] - 0.5) - 0.344 * (image[:, 2, :, :] - 0.5)\n",
        "    rgb[:, 2, :, :] = image[:, 0, :, :] + 1.773 * (image[:, 2, :, :] - 0.5)\n",
        "    return rgb"
      ],
      "metadata": {
        "id": "C5doRixz8FmR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rgb_1 = ycrcb_2_rgb(ycrcb_image_1)\n",
        "rgb_1 = rgb_1.squeeze(0)\n",
        "rgb_1 = rgb_1.swapaxes(0,2)\n",
        "rgb_1.size()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLAC_08M9qlY",
        "outputId": "6d68e903-6d61-492a-9a0c-10d11ee754e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1001, 1001, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_np_array_1 = plt.imread('/content/image2.png')\n",
        "np_arr = rgb_1.cpu().detach().numpy()\n",
        "plt.imshow(np_arr)"
      ],
      "metadata": {
        "id": "ckQei305_GqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ycrcb_image_2.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA--hFGeBfj8",
        "outputId": "471d576a-54a6-4a9d-fe5b-8a1daf9f3eba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 1001, 1001])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.MSELoss()\n",
        "input = torch.zeros(1,3,1001,1001,requires_grad=True)\n",
        "target = ycrcb_image_1\n",
        "output = loss(input, target)\n",
        "output.backward()"
      ],
      "metadata": {
        "id": "KEW9bUhoAwd0"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(params=,lr=0.001)\n",
        "input = torch.zeros(1,3,1001,1001,requires_grad=True)\n",
        "niter = 10\n",
        "for _ in range(0, niter):\n",
        "\toptimizer.zero_grad()\n",
        "\tloss = loss(input, target)\n",
        "\tloss.backward()\n",
        "\toptimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "KT5L9d8cDm-C",
        "outputId": "97b2ac5e-69dc-4331-c499-cd1f8ec03399"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-d1939620bf5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mycrcb_image_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mniter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize)\u001b[0m\n\u001b[1;32m     79\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     80\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad, maximize=maximize)\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     40\u001b[0m             raise TypeError(\"params argument given to the optimizer should be \"\n\u001b[1;32m     41\u001b[0m                             \u001b[0;34m\"an iterable of Tensors or dicts, but got \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                             torch.typename(params))\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor"
          ]
        }
      ]
    }
  ]
}